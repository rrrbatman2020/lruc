## Disclaymer

Добрый день.

Я забыл задать какие-нибудь уточняющие вопросы, поэтому написал так, как себе это представляю.

Времени было мало, поэтому я ставил перед собой больше задачу дописать всё целиком, чем написать "надёжно", "производительно", "элегантно", "минималистично" и даже "без багов". Также есть проблемы с пояснениями в коде и сообщениями об ошибках.

## Getting started
```
$ git clone https://github.com/rrrbatman2020/lruc.git
$ cd lruc
$ make
$ ./lruc "http://speedtest.tele2.net/50MB.zip" "./output"
```

## Что и как примерно работает

Для общения с сервером открывается TCP соединение посредством сокета в блокирубщем режиме.
Через данное соединение идёт обмен по протоколу HTTP, при чём HTTP соединение запрашивается персистентное (`Connection: Keep-Alive`).

Перед тем, как скачать непосредственно контент, выполняется `HEAD` запрос для выяснения размера файла и возможностей сервера.
Можно сразу сказать, что варианты скачивать контент неизвестного размера (без `Content-Length`) вроде:
1. Читать до тех пор, пока сервер на закроет соедениение
2. Использовать [chunked transfer encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding)

я не поддерживал.

### Поддерживается два варианта скачать контент:
#### Обычным `GET` запросом

1. Отправляется `GET` запрос. 
2. Ответ от сервера накапливается в буфер ограниченного размера.
3. При заполнении буффера — он флашится в файл в том же потоке.
4. Выполняем пункты 2-3 пока не прочитаем всё.

В случае возникновения каких-либо ошибок, есть попытки повторить весь запрос целиком.

#### `GET` запросом с [byte serving](https://en.wikipedia.org/wiki/Byte_serving)

1. Скачивание всего файла делится на чанки разумного размера (мегабайты).
2. Пробуем скачать чанк, для этого отправляем `GET` запрос с заголовком 'Range'. 
3. Читаем целиком ответ от сервера, флашим ответ в файл в том же потоке.
4. Если чанк скачан успешно, переходим к следующему, иначе ретраится только скачивание последнего чанка.
5. Выполняем пункты 2-4 пока не прочитаем всё.

Что в теории позволяет скачивать большие файлы и не перекачивать его целиком из-за небольших проблем с соединением.

### Как работает непосредственно получение ответа 

Поскольку в `HTTP` суммарнй размер заголовков в ответе не ограничен, приходится делать следующим образом
1. Заведём буфер размера `N`
2. Считаем из сокета `N` байт, но удалять из сокета их не будем (`peek`)
3. Если в считанных `N` байт содержутся все заголовки и они заканчиваются в позиции `K`, то читаем из сокета `K` байт
4. Иначе читаем из сокета `N` байт, увеличиваем буфер в два раза и переходим к пункту 2.
5. Если уже имеет неадекватный размер (мегабайты), то считаем, что с таким сервером мы общаться не хотим и падаем.

После того, как считаны заголовки, из них вынимается `Content-Length` и уже понятным образом вычитывается конент.

## Как тестировал

Потестировать различные сценарии на предмет того, что вообще в теории может отвечать сервер и как нужно себя при этом вести — не успел.

Корректность работы в общем случае проверял качая файлы с некоторых HTTP серверов вроде http://speedtest.tele2.net и сравнивал побайтово с тем, что качает `curl`.

Тестирование проводилось на OS X 10.14, нормального linux под рукой нет.
Проверил сборку и пару раз позапускал на Ubuntu 19.04, g++ 8.3.0, glibc 2.29.

## Время скачивания

Решил также посмотреть, насколько медленнее получается качать файлы по сравнению с `curl`.
Выбрал [файл размером 50МБ](http://speedtest.tele2.net/50MB.zip), выполнил по 40 запусков `curl` и данной программой.

Среднее (и медианное) время скачивания в на моём домашнем интернете от `curl` не отличается.
Что, в прочем, и говорит не очень много.
